"""
ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸
íŒŒì¼ ì—…ë¡œë“œë¶€í„° ë³´ê³ ì„œ ìƒì„±ê¹Œì§€ ì „ì²´ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸
"""

import pytest
import pandas as pd
import numpy as np
import tempfile
import os
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any

# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ëª¨ë“ˆ import
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from src.core.data_processor import DataProcessor
    from src.core.dynamic_dashboard_engine import DynamicDashboardEngine
    from src.core.data_models import TestResult, ProjectSummary, Standard
    from src.components.optimized_chart_renderer import OptimizedChartRenderer
    from src.utils.performance_optimizer import PerformanceOptimizer
    from src.utils.error_handler import ErrorHandler
    from src.utils.file_validator import FileValidator
except ImportError:
    # ëŒ€ì²´ import ê²½ë¡œ
    sys.path.append('src')
    from core.data_processor import DataProcessor
    from core.dynamic_dashboard_engine import DynamicDashboardEngine
    from core.data_models import TestResult, ProjectSummary, Standard
    from components.optimized_chart_renderer import OptimizedChartRenderer
    from utils.performance_optimizer import PerformanceOptimizer
    from utils.error_handler import ErrorHandler
    from utils.file_validator import FileValidator


class TestCompleteWorkflow:
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    @pytest.fixture
    def sample_excel_data(self):
        """í…ŒìŠ¤íŠ¸ìš© ì—‘ì…€ ë°ì´í„° ìƒì„±"""
        data = {
            'No.': list(range(1, 101)),
            'ì‹œë£Œëª…': [f'ì‹œë£Œ_{i % 10 + 1}' for i in range(100)],
            'ë¶„ì„ë²ˆí˜¸': [f'25A{i:05d}' for i in range(100)],
            'ì‹œí—˜í•­ëª©': np.random.choice([
                'ì•„í¬ë¦´ë¡œë‚˜ì´íŠ¸ë¦´', 'N-ë‹ˆíŠ¸ë¡œì¡°ë‹¤ì´ë©”í‹¸ì•„ë¯¼', 'ë²¤ì  ', 
                'í†¨ë£¨ì—”', 'í¬ì‹¤ë Œ'
            ], 100),
            'ì‹œí—˜ë‹¨ìœ„': ['mg/L'] * 100,
            'ê²°ê³¼(ì„±ì ì„œ)': [
                'ë¶ˆê²€ì¶œ' if np.random.random() < 0.3 
                else f'{np.random.uniform(0, 0.01):.6f}' 
                for _ in range(100)
            ],
            'ì‹œí—˜ìì…ë ¥ê°’': np.random.uniform(0, 0.01, 100),
            'ê¸°ì¤€ëŒ€ë¹„ ì´ˆê³¼ì—¬ë¶€ (ì„±ì ì„œ)': np.random.choice(['ì í•©', 'ë¶€ì í•©'], 100, p=[0.7, 0.3]),
            'ì‹œí—˜ì': np.random.choice(['ê¹€í™”ë¹ˆ', 'ì´í˜„í’', 'ë°•ë¯¼ìˆ˜'], 100),
            'ì‹œí—˜í‘œì¤€': ['EPA 524.2'] * 100,
            'ê¸°ì¤€': ['0.0006 mg/L ì´í•˜'] * 100,
            'ì…ë ¥ì¼ì‹œ': [
                (datetime.now() - timedelta(days=np.random.randint(0, 30))).strftime('%Y-%m-%d %H:%M')
                for _ in range(100)
            ]
        }
        return pd.DataFrame(data)
    
    @pytest.fixture
    def temp_excel_file(self, sample_excel_data):
        """ì„ì‹œ ì—‘ì…€ íŒŒì¼ ìƒì„±"""
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
            sample_excel_data.to_excel(tmp_file.name, index=False)
            yield tmp_file.name
        
        # ì •ë¦¬
        if os.path.exists(tmp_file.name):
            os.unlink(tmp_file.name)
    
    @pytest.fixture
    def large_excel_data(self):
        """ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (5000í–‰)"""
        size = 5000
        data = {
            'No.': list(range(1, size + 1)),
            'ì‹œë£Œëª…': [f'ì‹œë£Œ_{i % 100 + 1}' for i in range(size)],
            'ë¶„ì„ë²ˆí˜¸': [f'25A{i:05d}' for i in range(size)],
            'ì‹œí—˜í•­ëª©': np.random.choice([
                'ì•„í¬ë¦´ë¡œë‚˜ì´íŠ¸ë¦´', 'N-ë‹ˆíŠ¸ë¡œì¡°ë‹¤ì´ë©”í‹¸ì•„ë¯¼', 'ë²¤ì  ', 
                'í†¨ë£¨ì—”', 'í¬ì‹¤ë Œ', 'ì—í‹¸ë²¤ì  ', 'ìŠ¤í‹°ë Œ', 'í´ë¡œë¡œí¬ë¦„'
            ], size),
            'ì‹œí—˜ë‹¨ìœ„': ['mg/L'] * size,
            'ê²°ê³¼(ì„±ì ì„œ)': [
                'ë¶ˆê²€ì¶œ' if np.random.random() < 0.2 
                else f'{np.random.uniform(0, 0.02):.6f}' 
                for _ in range(size)
            ],
            'ì‹œí—˜ìì…ë ¥ê°’': np.random.uniform(0, 0.02, size),
            'ê¸°ì¤€ëŒ€ë¹„ ì´ˆê³¼ì—¬ë¶€ (ì„±ì ì„œ)': np.random.choice(['ì í•©', 'ë¶€ì í•©'], size, p=[0.6, 0.4]),
            'ì‹œí—˜ì': np.random.choice(['ê¹€í™”ë¹ˆ', 'ì´í˜„í’', 'ë°•ë¯¼ìˆ˜', 'ìµœì˜í¬', 'ì •ìˆ˜ì§„'], size),
            'ì‹œí—˜í‘œì¤€': np.random.choice(['EPA 524.2', 'EPA 525.2', 'House Method'], size),
            'ê¸°ì¤€': ['0.0006 mg/L ì´í•˜'] * size,
            'ì…ë ¥ì¼ì‹œ': [
                (datetime.now() - timedelta(days=np.random.randint(0, 90))).strftime('%Y-%m-%d %H:%M')
                for _ in range(size)
            ]
        }
        return pd.DataFrame(data)
    
    @pytest.fixture
    def temp_large_excel_file(self, large_excel_data):
        """ëŒ€ìš©ëŸ‰ ì„ì‹œ ì—‘ì…€ íŒŒì¼ ìƒì„±"""
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
            large_excel_data.to_excel(tmp_file.name, index=False)
            yield tmp_file.name
        
        # ì •ë¦¬
        if os.path.exists(tmp_file.name):
            os.unlink(tmp_file.name)
    
    def test_complete_workflow_small_file(self, temp_excel_file):
        """ì†Œê·œëª¨ íŒŒì¼ ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # 1. íŒŒì¼ ê²€ì¦
        validator = FileValidator()
        validation_result = validator.validate_file(temp_excel_file)
        assert validation_result['is_valid'], f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {validation_result['errors']}"
        
        # 2. ë°ì´í„° ì²˜ë¦¬
        processor = DataProcessor()
        start_time = time.time()
        test_results = processor.parse_excel_file(temp_excel_file)
        parse_time = time.time() - start_time
        
        assert len(test_results) > 0, "íŒŒì‹±ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
        assert parse_time < 5.0, f"íŒŒì‹± ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {parse_time:.2f}ì´ˆ"
        
        # 3. í”„ë¡œì íŠ¸ ìš”ì•½ ìƒì„±
        project_name = "TEST_PROJECT"
        summary = processor.get_project_summary(project_name, test_results)
        
        assert summary.project_name == project_name
        assert summary.total_tests == len(test_results)
        assert 0 <= summary.violation_rate <= 100
        
        # 4. ëŒ€ì‹œë³´ë“œ ì—”ì§„ ì´ˆê¸°í™” ë° ì—…ë°ì´íŠ¸
        dashboard_engine = DynamicDashboardEngine(processor)
        dashboard_engine.update_dashboard(test_results, "test_file.xlsx")
        
        assert dashboard_engine.is_dashboard_initialized()
        assert dashboard_engine.get_current_file() == "test_file.xlsx"
        
        # 5. KPI ë°ì´í„° ê²€ì¦
        kpi_data = dashboard_engine.get_kpi_data()
        assert kpi_data is not None
        assert kpi_data['total_tests'] == len(test_results)
        assert kpi_data['non_conforming_tests'] >= 0
        assert 0 <= kpi_data['non_conforming_rate'] <= 100
        
        # 6. ì°¨íŠ¸ ë Œë”ë§ í…ŒìŠ¤íŠ¸
        chart_renderer = OptimizedChartRenderer()
        
        start_time = time.time()
        donut_config = chart_renderer.generate_optimized_donut_chart(test_results)
        donut_time = time.time() - start_time
        
        start_time = time.time()
        bar_config = chart_renderer.generate_optimized_bar_chart(test_results)
        bar_time = time.time() - start_time
        
        assert donut_time < 1.0, f"ë„ë„› ì°¨íŠ¸ ìƒì„± ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {donut_time:.2f}ì´ˆ"
        assert bar_time < 1.0, f"ë§‰ëŒ€ ì°¨íŠ¸ ìƒì„± ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {bar_time:.2f}ì´ˆ"
        
        # 7. DataFrame ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        start_time = time.time()
        df = processor.export_to_dataframe(test_results)
        export_time = time.time() - start_time
        
        assert len(df) == len(test_results)
        assert export_time < 2.0, f"DataFrame ë‚´ë³´ë‚´ê¸° ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {export_time:.2f}ì´ˆ"
        
        print(f"âœ… ì†Œê·œëª¨ íŒŒì¼ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   - íŒŒì‹± ì‹œê°„: {parse_time:.3f}ì´ˆ")
        print(f"   - ë„ë„› ì°¨íŠ¸: {donut_time:.3f}ì´ˆ")
        print(f"   - ë§‰ëŒ€ ì°¨íŠ¸: {bar_time:.3f}ì´ˆ")
        print(f"   - ë‚´ë³´ë‚´ê¸°: {export_time:.3f}ì´ˆ")
    
    def test_complete_workflow_large_file(self, temp_large_excel_file):
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # 1. íŒŒì¼ ê²€ì¦
        validator = FileValidator()
        validation_result = validator.validate_file(temp_large_excel_file)
        assert validation_result['is_valid'], f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {validation_result['errors']}"
        
        # 2. ì„±ëŠ¥ ìµœì í™”ê¸° ì´ˆê¸°í™”
        optimizer = PerformanceOptimizer()
        
        # 3. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
        processor = DataProcessor()
        start_time = time.time()
        test_results = processor.parse_excel_file(temp_large_excel_file)
        parse_time = time.time() - start_time
        
        assert len(test_results) > 4000, "ëŒ€ìš©ëŸ‰ ë°ì´í„° íŒŒì‹± ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"
        assert parse_time < 30.0, f"ëŒ€ìš©ëŸ‰ íŒŒì¼ íŒŒì‹± ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {parse_time:.2f}ì´ˆ"
        
        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_usage = optimizer.memory_monitor.get_current_memory()
        assert memory_usage < 500, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤: {memory_usage:.1f}MB"
        
        # 5. ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
        dashboard_engine = DynamicDashboardEngine(processor)
        start_time = time.time()
        dashboard_engine.update_dashboard(test_results, "large_test_file.xlsx")
        dashboard_time = time.time() - start_time
        
        assert dashboard_time < 10.0, f"ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {dashboard_time:.2f}ì´ˆ"
        
        # 6. ìµœì í™”ëœ ì°¨íŠ¸ ë Œë”ë§ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
        chart_renderer = OptimizedChartRenderer()
        
        start_time = time.time()
        donut_config = chart_renderer.generate_optimized_donut_chart(test_results)
        donut_time = time.time() - start_time
        
        start_time = time.time()
        bar_config = chart_renderer.generate_optimized_bar_chart(test_results)
        bar_time = time.time() - start_time
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œë„ í•©ë¦¬ì ì¸ ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ì–´ì•¼ í•¨
        assert donut_time < 3.0, f"ëŒ€ìš©ëŸ‰ ë„ë„› ì°¨íŠ¸ ìƒì„± ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {donut_time:.2f}ì´ˆ"
        assert bar_time < 3.0, f"ëŒ€ìš©ëŸ‰ ë§‰ëŒ€ ì°¨íŠ¸ ìƒì„± ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {bar_time:.2f}ì´ˆ"
        
        # 7. ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ ì œí•œ í™•ì¸ (ìµœì í™” ê²€ì¦)
        donut_series = donut_config.get('series', [])
        bar_data = bar_config.get('series', [{}])[0].get('data', [])
        
        assert len(donut_series) <= 1000, f"ë„ë„› ì°¨íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {len(donut_series)}"
        assert len(bar_data) <= 1000, f"ë§‰ëŒ€ ì°¨íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤: {len(bar_data)}"
        
        # 8. ì„±ëŠ¥ ë³´ê³ ì„œ í™•ì¸
        performance_report = optimizer.get_performance_report()
        assert performance_report['total_operations'] > 0
        assert performance_report['success_rate'] > 90  # 90% ì´ìƒ ì„±ê³µë¥ 
        
        print(f"âœ… ëŒ€ìš©ëŸ‰ íŒŒì¼ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   - ë°ì´í„° í¬ê¸°: {len(test_results)}í–‰")
        print(f"   - íŒŒì‹± ì‹œê°„: {parse_time:.3f}ì´ˆ")
        print(f"   - ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸: {dashboard_time:.3f}ì´ˆ")
        print(f"   - ë„ë„› ì°¨íŠ¸: {donut_time:.3f}ì´ˆ")
        print(f"   - ë§‰ëŒ€ ì°¨íŠ¸: {bar_time:.3f}ì´ˆ")
        print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f}MB")
        print(f"   - ì„±ê³µë¥ : {performance_report['success_rate']:.1f}%")
    
    def test_error_handling_workflow(self):
        """ì—ëŸ¬ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        processor = DataProcessor()
        error_handler = ErrorHandler()
        
        # 1. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í…ŒìŠ¤íŠ¸
        with pytest.raises(Exception):
            processor.parse_excel_file("nonexistent_file.xlsx")
        
        # 2. ì˜ëª»ëœ í˜•ì‹ íŒŒì¼ í…ŒìŠ¤íŠ¸
        with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as tmp_file:
            tmp_file.write(b"This is not an Excel file")
            tmp_file.flush()
            
            try:
                with pytest.raises(Exception):
                    processor.parse_excel_file(tmp_file.name)
            finally:
                os.unlink(tmp_file.name)
        
        # 3. ë¹ˆ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        empty_data = pd.DataFrame()
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
            empty_data.to_excel(tmp_file.name, index=False)
            
            try:
                validation_result = processor.validate_data_structure(empty_data)
                assert not validation_result['is_valid']
                assert 'ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤' in str(validation_result['errors'])
            finally:
                os.unlink(tmp_file.name)
        
        # 4. í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ í…ŒìŠ¤íŠ¸
        incomplete_data = pd.DataFrame({
            'No.': [1, 2, 3],
            'ì‹œë£Œëª…': ['A', 'B', 'C']
            # í•„ìˆ˜ ì»¬ëŸ¼ë“¤ì´ ëˆ„ë½ë¨
        })
        
        validation_result = processor.validate_data_structure(incomplete_data)
        assert not validation_result['is_valid']
        assert 'í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½' in str(validation_result['errors'])
        
        print("âœ… ì—ëŸ¬ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    def test_caching_performance(self, temp_excel_file):
        """ìºì‹± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        processor = DataProcessor()
        optimizer = PerformanceOptimizer()
        
        # ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤)
        start_time = time.time()
        test_results1 = processor.parse_excel_file(temp_excel_file)
        first_call_time = time.time() - start_time
        
        # í”„ë¡œì íŠ¸ ìš”ì•½ ì²« ë²ˆì§¸ í˜¸ì¶œ
        start_time = time.time()
        summary1 = processor.get_project_summary("TEST_PROJECT", test_results1)
        first_summary_time = time.time() - start_time
        
        # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸ - ê°™ì€ ë°ì´í„°)
        start_time = time.time()
        summary2 = processor.get_project_summary("TEST_PROJECT", test_results1)
        second_summary_time = time.time() - start_time
        
        # ìºì‹œ íš¨ê³¼ ê²€ì¦
        assert second_summary_time < first_summary_time, "ìºì‹œ íš¨ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
        
        # ìºì‹œëœ ê²°ê³¼ê°€ ë™ì¼í•œì§€ í™•ì¸
        assert summary1.project_name == summary2.project_name
        assert summary1.total_tests == summary2.total_tests
        assert summary1.violation_rate == summary2.violation_rate
        
        # ìºì‹œ í†µê³„ í™•ì¸
        cache_stats = optimizer.cache.get_stats()
        assert cache_stats['size'] > 0, "ìºì‹œì— ë°ì´í„°ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        
        print(f"âœ… ìºì‹± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   - ì²« ë²ˆì§¸ ìš”ì•½ ìƒì„±: {first_summary_time:.3f}ì´ˆ")
        print(f"   - ë‘ ë²ˆì§¸ ìš”ì•½ ìƒì„±: {second_summary_time:.3f}ì´ˆ")
        print(f"   - ìºì‹œ íš¨ê³¼: {first_summary_time / max(second_summary_time, 0.001):.1f}ë°° ë¹ ë¦„")
        print(f"   - ìºì‹œ í¬ê¸°: {cache_stats['size']}")
    
    def test_memory_optimization(self, temp_large_excel_file):
        """ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        processor = DataProcessor()
        optimizer = PerformanceOptimizer()
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        initial_memory = optimizer.memory_monitor.get_current_memory()
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
        test_results = processor.parse_excel_file(temp_large_excel_file)
        
        # DataFrame ë³€í™˜ ë° ìµœì í™”
        df = processor.export_to_dataframe(test_results)
        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        optimized_df = optimizer.optimize_dataframe_memory(df)
        optimized_memory = optimized_df.memory_usage(deep=True).sum() / 1024 / 1024
        
        # ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ ê²€ì¦
        memory_reduction = (original_memory - optimized_memory) / original_memory * 100
        assert memory_reduction > 0, "ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_memory = optimizer.memory_monitor.get_current_memory()
        memory_increase = final_memory - initial_memory
        
        # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ì´ í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
        assert memory_increase < 200, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë§ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤: {memory_increase:.1f}MB"
        
        print(f"âœ… ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   - ì›ë³¸ DataFrame: {original_memory:.1f}MB")
        print(f"   - ìµœì í™”ëœ DataFrame: {optimized_memory:.1f}MB")
        print(f"   - ë©”ëª¨ë¦¬ ì ˆì•½: {memory_reduction:.1f}%")
        print(f"   - ì´ ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.1f}MB")
    
    def test_concurrent_processing(self, temp_excel_file):
        """ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        import threading
        import queue
        
        processor = DataProcessor()
        results_queue = queue.Queue()
        errors_queue = queue.Queue()
        
        def process_file(file_path, thread_id):
            try:
                start_time = time.time()
                test_results = processor.parse_excel_file(file_path)
                processing_time = time.time() - start_time
                
                results_queue.put({
                    'thread_id': thread_id,
                    'results_count': len(test_results),
                    'processing_time': processing_time
                })
            except Exception as e:
                errors_queue.put({
                    'thread_id': thread_id,
                    'error': str(e)
                })
        
        # 5ê°œ ìŠ¤ë ˆë“œë¡œ ë™ì‹œ ì²˜ë¦¬
        threads = []
        for i in range(5):
            thread = threading.Thread(target=process_file, args=(temp_excel_file, i))
            threads.append(thread)
            thread.start()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        for thread in threads:
            thread.join(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
        
        # ê²°ê³¼ ê²€ì¦
        results = []
        while not results_queue.empty():
            results.append(results_queue.get())
        
        errors = []
        while not errors_queue.empty():
            errors.append(errors_queue.get())
        
        assert len(results) == 5, f"ì¼ë¶€ ìŠ¤ë ˆë“œê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {len(results)}/5"
        assert len(errors) == 0, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {errors}"
        
        # ëª¨ë“  ìŠ¤ë ˆë“œê°€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ”ì§€ í™•ì¸
        first_result_count = results[0]['results_count']
        for result in results:
            assert result['results_count'] == first_result_count, "ìŠ¤ë ˆë“œë³„ ê²°ê³¼ê°€ ë‹¤ë¦…ë‹ˆë‹¤"
        
        avg_processing_time = sum(r['processing_time'] for r in results) / len(results)
        
        print(f"âœ… ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   - ì„±ê³µí•œ ìŠ¤ë ˆë“œ: {len(results)}/5")
        print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.3f}ì´ˆ")
        print(f"   - ê²°ê³¼ ì¼ê´€ì„±: âœ“")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_class = TestCompleteWorkflow()
    
    # ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    sample_data = test_class.sample_excel_data()
    
    with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
        sample_data.to_excel(tmp_file.name, index=False)
        
        try:
            print("ğŸ§ª ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
            test_class.test_complete_workflow_small_file(tmp_file.name)
            test_class.test_error_handling_workflow()
            test_class.test_caching_performance(tmp_file.name)
            test_class.test_concurrent_processing(tmp_file.name)
            print("ğŸ‰ ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            
        finally:
            if os.path.exists(tmp_file.name):
                os.unlink(tmp_file.name)