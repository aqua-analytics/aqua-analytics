"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸°ì™€ ì¡°ê±´ì—ì„œì˜ ì„±ëŠ¥ ì¸¡ì •
"""

import pytest
import pandas as pd
import numpy as np
import time
import tempfile
import os
import psutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple

# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ëª¨ë“ˆ import
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.core.data_processor import DataProcessor
from src.core.dynamic_dashboard_engine import DynamicDashboardEngine
from src.components.optimized_chart_renderer import OptimizedChartRenderer
from src.utils.performance_optimizer import PerformanceOptimizer


class TestPerformanceBenchmarks:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    # ì„±ëŠ¥ ê¸°ì¤€ (ì´ˆ)
    PERFORMANCE_THRESHOLDS = {
        'small_file_parse': 2.0,      # 100í–‰ íŒŒì‹±
        'medium_file_parse': 10.0,    # 1000í–‰ íŒŒì‹±
        'large_file_parse': 30.0,     # 5000í–‰ íŒŒì‹±
        'xlarge_file_parse': 60.0,    # 10000í–‰ íŒŒì‹±
        'chart_render_small': 1.0,    # ì†Œê·œëª¨ ì°¨íŠ¸ ë Œë”ë§
        'chart_render_large': 3.0,    # ëŒ€ê·œëª¨ ì°¨íŠ¸ ë Œë”ë§
        'dashboard_update': 5.0,      # ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
        'memory_limit_mb': 500        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ (MB)
    }
    
    def generate_test_data(self, size: int) -> pd.DataFrame:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        test_items = [
            'ì•„í¬ë¦´ë¡œë‚˜ì´íŠ¸ë¦´', 'N-ë‹ˆíŠ¸ë¡œì¡°ë‹¤ì´ë©”í‹¸ì•„ë¯¼', 'ë²¤ì  ', 'í†¨ë£¨ì—”', 
            'í¬ì‹¤ë Œ', 'ì—í‹¸ë²¤ì  ', 'ìŠ¤í‹°ë Œ', 'í´ë¡œë¡œí¬ë¦„', 'ì‚¬ì—¼í™”íƒ„ì†Œ', 
            'íŠ¸ë¦¬í´ë¡œë¡œì—í‹¸ë Œ', 'í…ŒíŠ¸ë¼í´ë¡œë¡œì—í‹¸ë Œ', '1,1,1-íŠ¸ë¦¬í´ë¡œë¡œì—íƒ„'
        ]
        
        testers = ['ê¹€í™”ë¹ˆ', 'ì´í˜„í’', 'ë°•ë¯¼ìˆ˜', 'ìµœì˜í¬', 'ì •ìˆ˜ì§„', 'ì´ë¯¼í˜¸', 'ë°•ì§€ì˜']
        standards = ['EPA 524.2', 'EPA 525.2', 'House Method', 'KS M 0124']
        
        data = {
            'No.': list(range(1, size + 1)),
            'ì‹œë£Œëª…': [f'ì‹œë£Œ_{i % 50 + 1}' for i in range(size)],
            'ë¶„ì„ë²ˆí˜¸': [f'25A{i:05d}' for i in range(size)],
            'ì‹œí—˜í•­ëª©': np.random.choice(test_items, size),
            'ì‹œí—˜ë‹¨ìœ„': ['mg/L'] * size,
            'ê²°ê³¼(ì„±ì ì„œ)': [
                'ë¶ˆê²€ì¶œ' if np.random.random() < 0.25 
                else f'{np.random.uniform(0, 0.02):.6f}' 
                for _ in range(size)
            ],
            'ì‹œí—˜ìì…ë ¥ê°’': np.random.uniform(0, 0.02, size),
            'ê¸°ì¤€ëŒ€ë¹„ ì´ˆê³¼ì—¬ë¶€ (ì„±ì ì„œ)': np.random.choice(['ì í•©', 'ë¶€ì í•©'], size, p=[0.65, 0.35]),
            'ì‹œí—˜ì': np.random.choice(testers, size),
            'ì‹œí—˜í‘œì¤€': np.random.choice(standards, size),
            'ê¸°ì¤€': np.random.choice([
                '0.0006 mg/L ì´í•˜', '0.001 mg/L ì´í•˜', '0.005 mg/L ì´í•˜', '0.01 mg/L ì´í•˜'
            ], size),
            'ì…ë ¥ì¼ì‹œ': [
                (datetime.now() - timedelta(days=np.random.randint(0, 365))).strftime('%Y-%m-%d %H:%M')
                for _ in range(size)
            ],
            'ì²˜ë¦¬ë°©ì‹': np.random.choice(['ë°˜ì˜¬ë¦¼', 'ì ˆì‚¬', 'ì˜¬ë¦¼'], size),
            'ê²°ê³¼ìœ í˜•': np.random.choice(['ìˆ˜ì¹˜í˜•', 'ë¬¸ìí˜•'], size, p=[0.8, 0.2]),
            'ì‹œí—˜ìê·¸ë£¹': np.random.choice(['ìœ ê¸°(ALL)', 'ë¬´ê¸°(ALL)', 'ë¯¸ìƒë¬¼'], size),
            'ìŠ¹ì¸ìš”ì²­ì—¬ë¶€': np.random.choice(['Y', 'N'], size, p=[0.9, 0.1]),
            'ì„±ì ì„œ ì¶œë ¥ì—¬ë¶€': np.random.choice(['Y', 'N'], size, p=[0.95, 0.05]),
            'KOLAS ì—¬ë¶€': np.random.choice(['Y', 'N'], size, p=[0.3, 0.7])
        }
        
        return pd.DataFrame(data)
    
    def create_temp_excel_file(self, data: pd.DataFrame) -> str:
        """ì„ì‹œ ì—‘ì…€ íŒŒì¼ ìƒì„±"""
        tmp_file = tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False)
        data.to_excel(tmp_file.name, index=False)
        tmp_file.close()
        return tmp_file.name
    
    def measure_performance(self, func, *args, **kwargs) -> Tuple[Any, Dict[str, float]]:
        """ì„±ëŠ¥ ì¸¡ì • í—¬í¼ í•¨ìˆ˜"""
        process = psutil.Process()
        
        # ì‹œì‘ ìƒíƒœ ì¸¡ì •
        start_time = time.time()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        start_cpu = process.cpu_percent()
        
        # í•¨ìˆ˜ ì‹¤í–‰
        result = func(*args, **kwargs)
        
        # ì¢…ë£Œ ìƒíƒœ ì¸¡ì •
        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        end_cpu = process.cpu_percent()
        
        metrics = {
            'execution_time': end_time - start_time,
            'memory_used': end_memory - start_memory,
            'peak_memory': end_memory,
            'cpu_usage': max(start_cpu, end_cpu)
        }
        
        return result, metrics
    
    @pytest.mark.parametrize("data_size", [100, 1000, 5000, 10000])
    def test_file_parsing_performance(self, data_size):
        """íŒŒì¼ íŒŒì‹± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ“Š íŒŒì¼ íŒŒì‹± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ - {data_size}í–‰")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = self.generate_test_data(data_size)
        temp_file = self.create_temp_excel_file(test_data)
        
        try:
            processor = DataProcessor()
            
            # ì„±ëŠ¥ ì¸¡ì •
            test_results, metrics = self.measure_performance(
                processor.parse_excel_file, temp_file
            )
            
            # ê²°ê³¼ ê²€ì¦
            assert len(test_results) > 0, "íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
            assert len(test_results) <= data_size, "íŒŒì‹± ê²°ê³¼ê°€ ì˜ˆìƒë³´ë‹¤ ë§ìŠµë‹ˆë‹¤"
            
            # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
            size_category = self._get_size_category(data_size)
            threshold_key = f'{size_category}_file_parse'
            threshold = self.PERFORMANCE_THRESHOLDS.get(threshold_key, 60.0)
            
            assert metrics['execution_time'] < threshold, \
                f"íŒŒì‹± ì‹œê°„ ì´ˆê³¼: {metrics['execution_time']:.2f}ì´ˆ > {threshold}ì´ˆ"
            
            assert metrics['peak_memory'] < self.PERFORMANCE_THRESHOLDS['memory_limit_mb'], \
                f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {metrics['peak_memory']:.1f}MB"
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"   âœ… íŒŒì‹± ì™„ë£Œ: {len(test_results)}í–‰")
            print(f"   â±ï¸  ì‹¤í–‰ ì‹œê°„: {metrics['execution_time']:.3f}ì´ˆ")
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {metrics['memory_used']:.1f}MB (í”¼í¬: {metrics['peak_memory']:.1f}MB)")
            print(f"   ğŸ–¥ï¸  CPU ì‚¬ìš©ë¥ : {metrics['cpu_usage']:.1f}%")
            print(f"   ğŸ“ˆ ì²˜ë¦¬ ì†ë„: {len(test_results) / metrics['execution_time']:.0f}í–‰/ì´ˆ")
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    def _get_size_category(self, size: int) -> str:
        """ë°ì´í„° í¬ê¸° ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        if size <= 100:
            return 'small'
        elif size <= 1000:
            return 'medium'
        elif size <= 5000:
            return 'large'
        else:
            return 'xlarge'
    
    @pytest.mark.parametrize("data_size", [100, 1000, 5000])
    def test_chart_rendering_performance(self, data_size):
        """ì°¨íŠ¸ ë Œë”ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ“ˆ ì°¨íŠ¸ ë Œë”ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ - {data_size}í–‰")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = self.generate_test_data(data_size)
        temp_file = self.create_temp_excel_file(test_data)
        
        try:
            processor = DataProcessor()
            test_results = processor.parse_excel_file(temp_file)
            
            chart_renderer = OptimizedChartRenderer()
            
            # ë„ë„› ì°¨íŠ¸ ì„±ëŠ¥ ì¸¡ì •
            donut_config, donut_metrics = self.measure_performance(
                chart_renderer.generate_optimized_donut_chart, test_results
            )
            
            # ë§‰ëŒ€ ì°¨íŠ¸ ì„±ëŠ¥ ì¸¡ì •
            bar_config, bar_metrics = self.measure_performance(
                chart_renderer.generate_optimized_bar_chart, test_results
            )
            
            # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
            size_category = 'small' if data_size <= 1000 else 'large'
            threshold = self.PERFORMANCE_THRESHOLDS[f'chart_render_{size_category}']
            
            assert donut_metrics['execution_time'] < threshold, \
                f"ë„ë„› ì°¨íŠ¸ ë Œë”ë§ ì‹œê°„ ì´ˆê³¼: {donut_metrics['execution_time']:.2f}ì´ˆ"
            
            assert bar_metrics['execution_time'] < threshold, \
                f"ë§‰ëŒ€ ì°¨íŠ¸ ë Œë”ë§ ì‹œê°„ ì´ˆê³¼: {bar_metrics['execution_time']:.2f}ì´ˆ"
            
            # ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ ì œí•œ í™•ì¸
            donut_points = len(donut_config.get('series', []))
            bar_points = len(bar_config.get('series', [{}])[0].get('data', []))
            
            assert donut_points <= 1000, f"ë„ë„› ì°¨íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ ì´ˆê³¼: {donut_points}"
            assert bar_points <= 1000, f"ë§‰ëŒ€ ì°¨íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ ì´ˆê³¼: {bar_points}"
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"   âœ… ì°¨íŠ¸ ë Œë”ë§ ì™„ë£Œ")
            print(f"   ğŸ© ë„ë„› ì°¨íŠ¸: {donut_metrics['execution_time']:.3f}ì´ˆ ({donut_points}ê°œ í¬ì¸íŠ¸)")
            print(f"   ğŸ“Š ë§‰ëŒ€ ì°¨íŠ¸: {bar_metrics['execution_time']:.3f}ì´ˆ ({bar_points}ê°œ í¬ì¸íŠ¸)")
            print(f"   ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {max(donut_metrics['peak_memory'], bar_metrics['peak_memory']):.1f}MB")
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    @pytest.mark.parametrize("data_size", [1000, 5000])
    def test_dashboard_update_performance(self, data_size):
        """ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ›ï¸ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ - {data_size}í–‰")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = self.generate_test_data(data_size)
        temp_file = self.create_temp_excel_file(test_data)
        
        try:
            processor = DataProcessor()
            test_results = processor.parse_excel_file(temp_file)
            
            dashboard_engine = DynamicDashboardEngine(processor)
            
            # ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì„±ëŠ¥ ì¸¡ì •
            _, metrics = self.measure_performance(
                dashboard_engine.update_dashboard, test_results, "test_file.xlsx"
            )
            
            # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
            threshold = self.PERFORMANCE_THRESHOLDS['dashboard_update']
            assert metrics['execution_time'] < threshold, \
                f"ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì‹œê°„ ì´ˆê³¼: {metrics['execution_time']:.2f}ì´ˆ"
            
            # KPI ë°ì´í„° ê²€ì¦
            kpi_data = dashboard_engine.get_kpi_data()
            assert kpi_data is not None, "KPI ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            assert kpi_data['total_tests'] == len(test_results), "KPI ë°ì´í„°ê°€ ì •í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"   âœ… ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            print(f"   â±ï¸  ì‹¤í–‰ ì‹œê°„: {metrics['execution_time']:.3f}ì´ˆ")
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {metrics['peak_memory']:.1f}MB")
            print(f"   ğŸ“Š KPI ë°ì´í„°: {kpi_data['total_tests']}ê±´ ì²˜ë¦¬")
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    def test_memory_optimization_effectiveness(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ í…ŒìŠ¤íŠ¸")
        
        # ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = self.generate_test_data(5000)
        temp_file = self.create_temp_excel_file(test_data)
        
        try:
            processor = DataProcessor()
            optimizer = PerformanceOptimizer()
            
            # ì›ë³¸ ë°ì´í„° ì²˜ë¦¬
            test_results = processor.parse_excel_file(temp_file)
            original_df = processor.export_to_dataframe(test_results)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            original_memory = original_df.memory_usage(deep=True).sum() / 1024 / 1024
            
            # ìµœì í™” ì ìš©
            optimized_df, optimization_metrics = self.measure_performance(
                optimizer.optimize_dataframe_memory, original_df
            )
            
            optimized_memory = optimized_df.memory_usage(deep=True).sum() / 1024 / 1024
            
            # ìµœì í™” íš¨ê³¼ ê²€ì¦
            memory_reduction = (original_memory - optimized_memory) / original_memory * 100
            assert memory_reduction > 0, "ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
            
            # ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
            assert len(original_df) == len(optimized_df), "ìµœì í™” í›„ ë°ì´í„° ì†ì‹¤ ë°œìƒ"
            assert list(original_df.columns) == list(optimized_df.columns), "ì»¬ëŸ¼ êµ¬ì¡° ë³€ê²½ë¨"
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"   âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
            print(f"   ğŸ“Š ì›ë³¸ í¬ê¸°: {original_memory:.1f}MB")
            print(f"   ğŸ“‰ ìµœì í™” í›„: {optimized_memory:.1f}MB")
            print(f"   ğŸ’¡ ì ˆì•½ë¥ : {memory_reduction:.1f}%")
            print(f"   â±ï¸  ìµœì í™” ì‹œê°„: {optimization_metrics['execution_time']:.3f}ì´ˆ")
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    def test_caching_performance_impact(self):
        """ìºì‹± ì„±ëŠ¥ ì˜í–¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ—„ï¸ ìºì‹± ì„±ëŠ¥ ì˜í–¥ í…ŒìŠ¤íŠ¸")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = self.generate_test_data(1000)
        temp_file = self.create_temp_excel_file(test_data)
        
        try:
            processor = DataProcessor()
            test_results = processor.parse_excel_file(temp_file)
            
            # ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤)
            _, first_metrics = self.measure_performance(
                processor.get_project_summary, "TEST_PROJECT", test_results
            )
            
            # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸)
            _, second_metrics = self.measure_performance(
                processor.get_project_summary, "TEST_PROJECT", test_results
            )
            
            # ìºì‹œ íš¨ê³¼ ê²€ì¦
            speedup = first_metrics['execution_time'] / max(second_metrics['execution_time'], 0.001)
            assert speedup > 2, f"ìºì‹œ íš¨ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {speedup:.1f}ë°°"
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"   âœ… ìºì‹± íš¨ê³¼ í™•ì¸")
            print(f"   ğŸŒ ì²« ë²ˆì§¸ í˜¸ì¶œ: {first_metrics['execution_time']:.3f}ì´ˆ")
            print(f"   ğŸš€ ë‘ ë²ˆì§¸ í˜¸ì¶œ: {second_metrics['execution_time']:.3f}ì´ˆ")
            print(f"   âš¡ ì†ë„ í–¥ìƒ: {speedup:.1f}ë°°")
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    def test_concurrent_processing_scalability(self):
        """ë™ì‹œ ì²˜ë¦¬ í™•ì¥ì„± í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ”„ ë™ì‹œ ì²˜ë¦¬ í™•ì¥ì„± í…ŒìŠ¤íŠ¸")
        
        import threading
        import queue
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = self.generate_test_data(500)  # ì‘ì€ í¬ê¸°ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        temp_file = self.create_temp_excel_file(test_data)
        
        try:
            processor = DataProcessor()
            
            def process_file_worker(file_path, results_queue, thread_id):
                start_time = time.time()
                try:
                    test_results = processor.parse_excel_file(file_path)
                    processing_time = time.time() - start_time
                    results_queue.put({
                        'thread_id': thread_id,
                        'success': True,
                        'processing_time': processing_time,
                        'results_count': len(test_results)
                    })
                except Exception as e:
                    results_queue.put({
                        'thread_id': thread_id,
                        'success': False,
                        'error': str(e),
                        'processing_time': time.time() - start_time
                    })
            
            # ë‹¤ì–‘í•œ ë™ì‹œì„± ë ˆë²¨ í…ŒìŠ¤íŠ¸
            for thread_count in [1, 2, 4, 8]:
                print(f"   ğŸ§µ {thread_count}ê°œ ìŠ¤ë ˆë“œ í…ŒìŠ¤íŠ¸")
                
                results_queue = queue.Queue()
                threads = []
                
                start_time = time.time()
                
                # ìŠ¤ë ˆë“œ ì‹œì‘
                for i in range(thread_count):
                    thread = threading.Thread(
                        target=process_file_worker,
                        args=(temp_file, results_queue, i)
                    )
                    threads.append(thread)
                    thread.start()
                
                # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
                for thread in threads:
                    thread.join(timeout=30)
                
                total_time = time.time() - start_time
                
                # ê²°ê³¼ ìˆ˜ì§‘
                results = []
                while not results_queue.empty():
                    results.append(results_queue.get())
                
                # ì„±ê³µë¥  í™•ì¸
                successful_results = [r for r in results if r.get('success', False)]
                success_rate = len(successful_results) / thread_count * 100
                
                assert success_rate >= 80, f"ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤: {success_rate:.1f}%"
                
                # í‰ê·  ì²˜ë¦¬ ì‹œê°„
                avg_processing_time = sum(r['processing_time'] for r in successful_results) / len(successful_results)
                
                print(f"      âœ… ì„±ê³µë¥ : {success_rate:.1f}%")
                print(f"      â±ï¸  ì´ ì‹œê°„: {total_time:.3f}ì´ˆ")
                print(f"      ğŸ“Š í‰ê·  ì²˜ë¦¬: {avg_processing_time:.3f}ì´ˆ")
                
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    def test_performance_regression(self):
        """ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ“‰ ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸")
        
        # ê¸°ì¤€ ì„±ëŠ¥ ë°ì´í„° (ì˜ˆìƒ ì„±ëŠ¥)
        baseline_performance = {
            'small_parse': 1.0,    # 100í–‰ íŒŒì‹± (ì´ˆ)
            'medium_parse': 5.0,   # 1000í–‰ íŒŒì‹± (ì´ˆ)
            'chart_render': 0.5,   # ì°¨íŠ¸ ë Œë”ë§ (ì´ˆ)
            'dashboard_update': 2.0 # ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ (ì´ˆ)
        }
        
        # í˜„ì¬ ì„±ëŠ¥ ì¸¡ì •
        current_performance = {}
        
        # ì†Œê·œëª¨ íŒŒì‹± í…ŒìŠ¤íŠ¸
        test_data = self.generate_test_data(100)
        temp_file = self.create_temp_excel_file(test_data)
        
        try:
            processor = DataProcessor()
            
            # íŒŒì‹± ì„±ëŠ¥
            _, metrics = self.measure_performance(processor.parse_excel_file, temp_file)
            current_performance['small_parse'] = metrics['execution_time']
            
            test_results = processor.parse_excel_file(temp_file)
            
            # ì°¨íŠ¸ ë Œë”ë§ ì„±ëŠ¥
            chart_renderer = OptimizedChartRenderer()
            _, metrics = self.measure_performance(
                chart_renderer.generate_optimized_donut_chart, test_results
            )
            current_performance['chart_render'] = metrics['execution_time']
            
            # ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì„±ëŠ¥
            dashboard_engine = DynamicDashboardEngine(processor)
            _, metrics = self.measure_performance(
                dashboard_engine.update_dashboard, test_results, "test.xlsx"
            )
            current_performance['dashboard_update'] = metrics['execution_time']
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
        
        # ì¤‘ê°„ ê·œëª¨ íŒŒì‹± í…ŒìŠ¤íŠ¸
        test_data = self.generate_test_data(1000)
        temp_file = self.create_temp_excel_file(test_data)
        
        try:
            _, metrics = self.measure_performance(processor.parse_excel_file, temp_file)
            current_performance['medium_parse'] = metrics['execution_time']
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
        
        # ì„±ëŠ¥ íšŒê·€ ê²€ì¦
        regression_threshold = 1.5  # 50% ì´ìƒ ëŠë ¤ì§€ë©´ íšŒê·€ë¡œ íŒë‹¨
        
        for operation, baseline in baseline_performance.items():
            current = current_performance.get(operation, 0)
            regression_ratio = current / baseline
            
            print(f"   ğŸ“Š {operation}: {current:.3f}ì´ˆ (ê¸°ì¤€: {baseline:.3f}ì´ˆ, ë¹„ìœ¨: {regression_ratio:.2f}x)")
            
            assert regression_ratio < regression_threshold, \
                f"ì„±ëŠ¥ íšŒê·€ ê°ì§€ - {operation}: {regression_ratio:.2f}ë°° ëŠë ¤ì§"
        
        print(f"   âœ… ì„±ëŠ¥ íšŒê·€ ì—†ìŒ í™•ì¸")


if __name__ == "__main__":
    # ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_class = TestPerformanceBenchmarks()
    
    print("ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_class.test_file_parsing_performance(100)
    test_class.test_file_parsing_performance(1000)
    test_class.test_chart_rendering_performance(1000)
    test_class.test_dashboard_update_performance(1000)
    test_class.test_memory_optimization_effectiveness()
    test_class.test_caching_performance_impact()
    test_class.test_concurrent_processing_scalability()
    test_class.test_performance_regression()
    
    print("ğŸ‰ ëª¨ë“  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")